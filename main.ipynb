{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b0196a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[2790]: Class CaptureDelegate is implemented in both /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x144d0e538) and /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1273ac860). One of the two will be used. Which one is undefined.\n",
      "objc[2790]: Class CVWindow is implemented in both /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x144d0e588) and /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x116110a68). One of the two will be used. Which one is undefined.\n",
      "objc[2790]: Class CVView is implemented in both /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x144d0e5b0) and /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x116110a90). One of the two will be used. Which one is undefined.\n",
      "objc[2790]: Class CVSlider is implemented in both /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x144d0e5d8) and /Users/odner/opt/miniconda3/envs/torch/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x116110ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecedc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data', 'images')\n",
    "EDIT_IMAGES_PATH = os.path.join('data', 'edit_images')\n",
    "CORDS_EMOTION_PATH = os.path.join('data', 'cords_emotion.npy')\n",
    "number_images = 30\n",
    "\n",
    "arr_cords_emotion = np.array([[\"name\", 'id', [('x', 'y', 'z')]]], dtype=object)\n",
    "np.save(CORDS_EMOTION_PATH, arr_cords_emotion)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a337af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictMediapipeInArray(emotion_id, name, face_landmarks):\n",
    "    global arr_cords_emotion\n",
    "    \n",
    "    arr_cords = list()\n",
    "    for item in face_landmarks.landmark:\n",
    "        arr_cords.append((item.x, item.y, item.z))\n",
    "        \n",
    "    with open(CORDS_EMOTION_PATH, 'rb') as file:\n",
    "        arr = np.load(file, allow_pickle=True)\n",
    "    with open(CORDS_EMOTION_PATH, 'wb') as file:\n",
    "        arr = np.append(arr, [[name, emotion_id, arr_cords]], axis = 0)\n",
    "        np.save(file, arr)\n",
    "\n",
    "    del arr_cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6657213",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0\n",
      "Image: 1\n",
      "Image: 2\n",
      "Image: 3\n",
      "Image: 4\n",
      "Image: 5\n",
      "Image: 6\n",
      "Image: 7\n",
      "Image: 8\n",
      "Image: 9\n",
      "Image: 10\n",
      "Image: 11\n",
      "Image: 12\n",
      "Image: 13\n",
      "Image: 14\n",
      "Image: 15\n",
      "Image: 16\n",
      "Image: 17\n",
      "Image: 18\n",
      "Image: 19\n",
      "Image: 20\n",
      "Image: 21\n",
      "Image: 22\n",
      "Image: 23\n",
      "Image: 24\n",
      "Image: 25\n",
      "Image: 26\n",
      "Image: 27\n",
      "Image: 28\n",
      "Image: 29\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        image.flags.writeable = False\n",
    "        # Меняем цветовую гамму\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Либа по посторойке координат на лице\n",
    "        results = face_mesh.process(image)\n",
    "        image.flags.writeable = True\n",
    "        # Возвращаем цвет обратно\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "               \n",
    "    cv2.imshow('image', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print(f'Image: {imgnum}')\n",
    "    idimg = str(uuid.uuid4())\n",
    "            \n",
    "    ret, image = cap.read()\n",
    "    \n",
    "    imgname = os.path.join(IMAGES_PATH, f'{idimg}.jpg')\n",
    "    cv2.imwrite(imgname, image)\n",
    "            \n",
    "    with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        image.flags.writeable = False\n",
    "        # Меняем цветовую гамму\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Либа по посторойке координат на лице\n",
    "        results = face_mesh.process(image)\n",
    "        image.flags.writeable = True\n",
    "        # Возвращаем цвет обратно\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "    \n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            dictMediapipeInArray(idimg, emotion_id, face_landmarks)\n",
    "\n",
    "    imgname = os.path.join(EDIT_IMAGES_PATH, f'{idimg}.jpg')\n",
    "    cv2.imwrite(imgname, image)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    cv2.imshow('image', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

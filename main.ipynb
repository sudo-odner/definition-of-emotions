{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b0196a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f3b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f07f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FILES = []\n",
    "emotion_key ={\n",
    "    0: 'none',\n",
    "    1: 'happy',\n",
    "    2: 'sadness',\n",
    "    3: 'angry',\n",
    "    4: 'disgust',\n",
    "    5: 'surprise',\n",
    "    6: 'fear',\n",
    "    7: 'neutral',\n",
    "    'happy': 1,\n",
    "    'sadness': 2,\n",
    "    'angry': 3,\n",
    "    'disgust': 4,\n",
    "    'surprise': 5,\n",
    "    'fear': 6,\n",
    "    'none': 0,\n",
    "    'neutral': 7,\n",
    "}\n",
    "\n",
    "\n",
    "CORDS_EMOTION_PATH = os.path.join('data', 'cords_emotion.npy')\n",
    "INDEX_EMOTION_PATH = os.path.join('data', 'index_emotion.npy')\n",
    "IMAGES_EMOTION_ID_PATH = os.path.join('data', 'images')\n",
    "EDIT_IMAGES_EMOTION_ID_PATH = os.path.join('data', 'edit')\n",
    "\n",
    "arr_cords_emotion = np.zeros((1, 478, 3))\n",
    "arr_index_emotion = np.zeros(1)\n",
    "np.save(CORDS_EMOTION_PATH, arr_cords_emotion)\n",
    "np.save(INDEX_EMOTION_PATH, arr_index_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a337af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmotionCordsArray(emotion_id, face_landmarks):\n",
    "    global arr_cords_emotion\n",
    "    \n",
    "    arr_cords = list()\n",
    "    for item in face_landmarks.landmark:\n",
    "        arr_cords.append([item.x, item.y, item.z])\n",
    "    \n",
    "    arr_cords = np.array(arr_cords)\n",
    "    \n",
    "    with open(CORDS_EMOTION_PATH, 'rb') as file:\n",
    "        arr_cords_emotion = np.load(file, allow_pickle=True)\n",
    "    with open(INDEX_EMOTION_PATH, 'rb') as file:\n",
    "        arr_index_emotion = np.load(file, allow_pickle=True)\n",
    "    \n",
    "    with open(CORDS_EMOTION_PATH, 'wb') as file:\n",
    "        arr_cords_emotion = np.append(arr_cords_emotion, [arr_cords], axis=0)\n",
    "        np.save(file, arr_cords_emotion)\n",
    "    with open(INDEX_EMOTION_PATH, 'wb') as file:\n",
    "        arr_index_emotion = np.append(arr_index_emotion, emotion_id)\n",
    "        np.save(file, arr_index_emotion)\n",
    "\n",
    "    del arr_cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d13ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m                 mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(image, face_landmarks, mp_face_mesh\u001b[38;5;241m.\u001b[39mFACEMESH_TESSELATION, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                                       mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_face_mesh_tesselation_style())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#                 cv2.imwrite(f'{EDIT_IMAGES_EMOTION_ID_PATH}/{index}.png', image)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m                 \u001b[43mEmotionCordsArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43memotion_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43memotion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_landmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(index)\n",
      "Cell \u001b[0;32mIn [5], line 17\u001b[0m, in \u001b[0;36mEmotionCordsArray\u001b[0;34m(emotion_id, face_landmarks)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(CORDS_EMOTION_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     16\u001b[0m     arr_cords_emotion \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(arr_cords_emotion, [arr_cords], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr_cords_emotion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INDEX_EMOTION_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     19\u001b[0m     arr_index_emotion \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(arr_index_emotion, emotion_id)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/npyio.py:502\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    501\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/format.py:689\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 689\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    692\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    693\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "    for emotion in os.listdir(IMAGES_EMOTION_ID_PATH):\n",
    "        if emotion == '.DS_Store':\n",
    "            continue\n",
    "            \n",
    "        for index, file in enumerate(os.listdir(f'{IMAGES_EMOTION_ID_PATH}/{emotion}')):\n",
    "\n",
    "            image = cv2.imread(f'{IMAGES_EMOTION_ID_PATH}/{emotion}/{file}')\n",
    "            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                face_landmarks = results.multi_face_landmarks[0]\n",
    "                mp_drawing.draw_landmarks(image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "                                      mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "#                 cv2.imwrite(f'{EDIT_IMAGES_EMOTION_ID_PATH}/{index}.png', image)\n",
    "                EmotionCordsArray(emotion_key[emotion], face_landmarks)\n",
    "            if index % 100 == 0:\n",
    "                print(index)\n",
    "        print(f'EMOTION: {emotion}, Completed')\n",
    "    print('COMPELTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f55a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]]]\n"
     ]
    }
   ],
   "source": [
    "ff = np.zeros((2, 6, 3))\n",
    "d = [[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3]]\n",
    "ff = np.append(ff, [d], axis=0)\n",
    "print(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_id = int(input())\n",
    "while emotion_id not in [0,1,2,3,4,5,6]:\n",
    "    emotion_id = int(input())\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 121: # exit on ESC\n",
    "        os.listdir(IMAGES_EMOTION_ID_PATH)[-1]\n",
    "        cv2.imwrite(f'{IMAGES_EMOTION_ID_PATH}/{str((os.listdir(IMAGES_EMOTION_ID_PATH)[-1]) + 1)}.png', image)\n",
    "        print('Good')\n",
    "        break\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "                                      mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2fdbb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# for imgnum in range(number_images):\n",
    "#     print(f'Image: {imgnum}')\n",
    "#     idimg = str(uuid.uuid4())\n",
    "            \n",
    "#     ret, image = cap.read()\n",
    "    \n",
    "#     imgname = os.path.join(IMAGES_PATH, f'{idimg}.jpg')\n",
    "#     cv2.imwrite(imgname, image)\n",
    "            \n",
    "#     with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "#         image.flags.writeable = False\n",
    "#         # Меняем цветовую гамму\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         # Либа по посторойке координат на лице\n",
    "#         results = face_mesh.process(image)\n",
    "#         image.flags.writeable = True\n",
    "#         # Возвращаем цвет обратно\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "    \n",
    "#         if results.multi_face_landmarks:\n",
    "#             face_landmarks = results.multi_face_landmarks[0]\n",
    "#             mp_drawing.draw_landmarks(image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION, None,\n",
    "#                                       mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "#         dictMediapipeInArray(idimg, emotion_id, face_landmarks)\n",
    "\n",
    "#     imgname = os.path.join(EDIT_IMAGES_PATH, f'{idimg}.jpg')\n",
    "#     cv2.imwrite(imgname, image)\n",
    "#     time.sleep(0.5)\n",
    "\n",
    "#     cv2.imshow('image', image)\n",
    "\n",
    "#     if cv2.waitKey(5) & 0xFF == 27: # exit on ESC\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c6542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "\n",
    "# Запуск видеопотока\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Подключение детектора, настроеного на поиск человеческих лиц\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Получение изображения из видеопотока\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    # Конвертирование изображения в черно-белое\n",
    "    grayFrame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Обнаружение лиц и построение прямоугольного контура\n",
    "    faces = detector(grayFrame)\n",
    "\n",
    "    # Обход списка всех лиц попавших на изображение\n",
    "    for face in faces:\n",
    "\n",
    "        # Выводим количество лиц на изображении\n",
    "        cv2.putText(image, \"{} face(s) found\".format(len(faces)), (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Получение координат вершин прямоугольника и его построение на изображении\n",
    "        x1 = face.left()\n",
    "        y1 = face.top()\n",
    "        x2 = face.right()\n",
    "        y2 = face.bottom()\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "        \n",
    "        # Получение координат контрольных точек и их построение на изображении\n",
    "        landmarks = predictor(grayFrame, face)\n",
    "        for n in range(0, 68):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            cv2.circle(image, (x, y), 3, (255, 0, 0), -1)\n",
    "\n",
    "    cv2.putText(image, \"Press ESC to close frame\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Вывод преобразованного изображения\n",
    "    cv2.imshow(\"Frame\", image)\n",
    "\n",
    "    # Для выхода из цикла нажать ESC\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
